{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c562f326928442781ebae6a29219168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "all-mpnet-base-v2"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Embedding Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_edb4c0f3e61a44c5ad376d2fdadcf5c3",
            "style": "IPY_MODEL_e2c63d2bd734489299cf38a978845fa0"
          }
        },
        "edb4c0f3e61a44c5ad376d2fdadcf5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2c63d2bd734489299cf38a978845fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67b3521cf9b948d5aaf6d26eccc717c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CPU",
              "AUTO"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Device:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_04381284025041d398893a622ca44d0f",
            "style": "IPY_MODEL_d1605a1e850a4368b980a614ec593f3c"
          }
        },
        "04381284025041d398893a622ca44d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1605a1e850a4368b980a614ec593f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70a540eb1b1e46678ce80eabb4012581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CPU",
              "AUTO"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Device:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_5e34f8b4019b40c091c6598b8fed852f",
            "style": "IPY_MODEL_948dc72ed7b046869891e10cba1628d3"
          }
        },
        "5e34f8b4019b40c091c6598b8fed852f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "948dc72ed7b046869891e10cba1628d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d7d7b6b6acb4e318d093e09253f4be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "INT4",
              "INT8"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model to run:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_2e676ae7ee4e4639ad218d7d9a01592c",
            "style": "IPY_MODEL_ecde56378d3440c682319f05d593d840"
          }
        },
        "2e676ae7ee4e4639ad218d7d9a01592c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecde56378d3440c682319f05d593d840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf4ada5ada7b44b9b33e442de49d116d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "tiny-llama-1b-chat",
              "minicpm-2b-dpo",
              "red-pajama-3b-chat",
              "llama-2-chat-7b",
              "mpt-7b-chat",
              "qwen-7b-chat",
              "chatglm3-6b",
              "mistral-7b",
              "zephyr-7b-beta",
              "neural-chat-7b-v3-1",
              "notus-7b-v1",
              "youri-7b-chat"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "LLM Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 7,
            "layout": "IPY_MODEL_517cb1a415c848a6b70556f69808f0d1",
            "style": "IPY_MODEL_7f8c774fa13b45e2a6b733169314058b"
          }
        },
        "517cb1a415c848a6b70556f69808f0d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f8c774fa13b45e2a6b733169314058b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c33c1237f62c4c64a8fdb7bbe7cb8dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare INT4 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_cacc0d8ee5304b13b17dfae5c7e247b1",
            "style": "IPY_MODEL_c2f51ea020a941b7b227d98daca6145e",
            "value": false
          }
        },
        "cacc0d8ee5304b13b17dfae5c7e247b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2f51ea020a941b7b227d98daca6145e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26077142ff754fda868a445a4f1bbb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare INT8 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_dcc0eb31b4704ba0a9845bf51b9e07b5",
            "style": "IPY_MODEL_81eabc31d5bd46589ab65ed2c610e072",
            "value": true
          }
        },
        "dcc0eb31b4704ba0a9845bf51b9e07b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81eabc31d5bd46589ab65ed2c610e072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcb0ad51baba4e8185336430e04f2983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare FP16 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2388e9a1c5484125b2d96876d8c097f0",
            "style": "IPY_MODEL_fe8abf90166c444fbeba7c2213d06d7a",
            "value": true
          }
        },
        "2388e9a1c5484125b2d96876d8c097f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe8abf90166c444fbeba7c2213d06d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZGNtrgs9dJ4",
        "outputId": "cac273c4-37b2-4175-bccc-32ca30de8102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         52 bits physical, 57 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  22\n",
            "  On-line CPU(s) list:   0-21\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) Platinum 8481C CPU @ 2.70GHz\n",
            "    CPU family:          6\n",
            "    Model:               143\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  11\n",
            "    Socket(s):           1\n",
            "    Stepping:            8\n",
            "    BogoMIPS:            5399.99\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_\n",
            "                         enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rtm avx512f a\n",
            "                         vx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw\n",
            "                          avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 arat avx512vb\n",
            "                         mi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_\n",
            "                         vpopcntdq rdpid cldemote movdiri movdir64b fsrm md_clear serialize amx_bf16\n",
            "                          avx512_fp16 amx_tile amx_int8 arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   528 KiB (11 instances)\n",
            "  L1i:                   352 KiB (11 instances)\n",
            "  L2:                    22 MiB (11 instances)\n",
            "  L3:                    105 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0-21\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Not affected\n",
            "  Mds:                   Not affected\n",
            "  Meltdown:              Not affected\n",
            "  Mmio stale data:       Not affected\n",
            "  Retbleed:              Not affected\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\n",
            "  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
            "  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW se\n",
            "                         quence\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Not affected\n"
          ]
        }
      ],
      "source": [
        "!lscpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y40Ov1f9jq8",
        "outputId": "960d27a9-8c89-4d0d-d0b6-36f32b0acd15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.1.58+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall -q -y openvino-dev openvino openvino-nightly\n",
        "%pip install --verbose -q -U --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
        "\"optimum-intel\"\\\n",
        "\"nncf>=2.8.0\"\\\n",
        "\"datasets\"\\\n",
        "\"accelerate\"\\\n",
        "\"openvino-nightly\"\\\n",
        "\"gradio\"\\\n",
        "\"einops\" \"onnx\" \"chromadb\" \"sentence_transformers\" \"langchain\" \"langchainhub\" \"transformers>=4.34.0\" \"unstructured\" \"scikit-learn\" \"python-docx\" \"pypdf\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFUWraVe97g1",
        "outputId": "12abbead-2137-4f53-dca5-d278db2b806b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping openvino-dev as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping openvino as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping openvino-nightly as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
            "Collecting optimum-intel\n",
            "  Downloading optimum_intel-1.15.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.7/171.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nncf>=2.8.0\n",
            "  Downloading nncf-2.8.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openvino-nightly\n",
            "  Downloading openvino_nightly-2024.1.0.dev20240221-14548-cp310-cp310-manylinux2014_x86_64.whl (38.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-4.19.1-py3-none-any.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.8-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchainhub\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Requirement already satisfied: transformers>=4.34.0 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting transformers>=4.34.0\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m137.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured\n",
            "  Downloading unstructured-0.12.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m161.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum-intel) (2.1.0+cu121)\n",
            "Collecting optimum>=1.17.0 (from optimum-intel)\n",
            "  Downloading optimum-1.17.1-py3-none-any.whl (407 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.34.0\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m150.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from optimum-intel) (0.1.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from optimum-intel) (1.11.4)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (4.19.2)\n",
            "Collecting jstyleson>=0.0.2 (from nncf>=2.8.0)\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (8.4.0)\n",
            "Collecting networkx<=3.1,>=2.6 (from nncf>=2.8.0)\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m159.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja<1.11,>=1.10.0.post2 (from nncf>=2.8.0)\n",
            "  Downloading ninja-1.10.2.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27,>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (1.23.5)\n",
            "Collecting openvino-telemetry>=2023.2.0 (from nncf>=2.8.0)\n",
            "  Downloading openvino_telemetry-2023.2.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (23.2)\n",
            "Requirement already satisfied: pandas<2.2,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (5.9.5)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (1.4.2)\n",
            "Collecting pymoo>=0.6.0.1 (from nncf>=2.8.0)\n",
            "  Downloading pymoo-0.6.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.5.2 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (13.7.0)\n",
            "Collecting texttable>=1.6.3 (from nncf>=2.8.0)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.8.0) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Collecting pyarrow>=12.0.0 (from datasets)\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.10.0 (from gradio)\n",
            "  Downloading gradio_client-0.10.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.1.7 (from gradio)\n",
            "  Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m177.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio-client==0.10.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m156.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.4.2-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m168.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m137.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.25)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
            "  Downloading langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.24 (from langchain)\n",
            "  Downloading langchain_core-0.1.25-py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
            "  Downloading langsmith-0.1.5-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.0) (2023.12.25)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m176.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting unstructured-client>=0.15.1 (from unstructured)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf>=2.8.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf>=2.8.0) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf>=2.8.0) (0.17.1)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2,>=1.1.5->nncf>=2.8.0) (2023.4)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.1)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.10/dist-packages (from pymoo>=0.6.0.1->nncf>=2.8.0) (1.6.2)\n",
            "Collecting cma==3.2.2 (from pymoo>=0.6.0.1->nncf>=2.8.0)\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alive-progress (from pymoo>=0.6.0.1->nncf>=2.8.0)\n",
            "  Downloading alive_progress-3.1.5-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nncf>=2.8.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nncf>=2.8.0) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum-intel) (2.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading https://download.pytorch.org/whl/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m162.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Collecting httpcore==1.* (from httpx->gradio)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.2.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.4->pymoo>=0.6.0.1->nncf>=2.8.0) (0.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nncf>=2.8.0) (0.1.2)\n",
            "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers[sentencepiece]>=4.26.0 (from optimum>=1.17.0->optimum-intel)\n",
            "  Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0.1->nncf>=2.8.0)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress->pymoo>=0.6.0.1->nncf>=2.8.0)\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: jstyleson, pypika, ffmpy, langdetect, grapheme\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2385 sha256=868b0b561c5b34035b47d3b1c1cab54c7fcb20a7652d3f2c63d17933af7b02aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/51/c6/a1e751db88203e11c6d9ffe4683ca3d8c14b1479639bec1006\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=54122469e856012214f3e081e9a2383ca18708e70097c419a65e0403be8c1fb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=46a7e66c86761964790149482c6ce57ac7fd8528db480ec354612824bd69bdd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=b6d44426ff2c4aaa9a4b4653508e5347003143331e0001a412fbcd60915adf6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210078 sha256=67883cba62188e1cdcd59f5e43d3ff85e819416189e405f2aa69274587f2954d\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e1/49/37e6bde9886439057450c494a79b0bef8bbe897a54aebfc757\n",
            "Successfully built jstyleson pypika ffmpy langdetect grapheme\n",
            "Installing collected packages: texttable, pypika, pydub, openvino-telemetry, ninja, monotonic, mmh3, jstyleson, grapheme, filetype, ffmpy, websockets, uvloop, types-requests, tomlkit, shellingham, semantic-version, ruff, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, python-docx, pypdf, pyarrow, pulsar-client, overrides, orjson, openvino-nightly, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, onnx, networkx, mypy-extensions, marshmallow, langdetect, jsonpointer, jsonpath-python, importlib-metadata, humanfriendly, httptools, h11, emoji, einops, dill, deprecated, colorama, cma, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, about-time, watchfiles, uvicorn, typing-inspect, starlette, scikit-learn, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, multiprocess, langchainhub, jsonpatch, httpcore, coloredlogs, alive-progress, pymoo, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, httpx, fastapi, dataclasses-json-speakeasy, dataclasses-json, accelerate, unstructured-client, transformers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, nncf, langchain-core, gradio-client, datasets, unstructured, sentence_transformers, opentelemetry-instrumentation-fastapi, langchain-community, gradio, optimum, langchain, chromadb, optimum-intel\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed about-time-4.2.1 accelerate-0.27.2 aiofiles-23.2.1 alive-progress-3.1.5 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 cma-3.2.2 colorama-0.4.6 coloredlogs-15.0.1 dataclasses-json-0.6.4 dataclasses-json-speakeasy-0.5.11 datasets-2.17.1 deprecated-1.2.14 dill-0.3.8 einops-0.7.0 emoji-2.10.1 fastapi-0.109.2 ffmpy-0.3.2 filetype-1.2.0 gradio-4.19.1 gradio-client-0.10.0 grapheme-0.6.0 h11-0.14.0 httpcore-1.0.4 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-2.4 jstyleson-0.0.2 kubernetes-29.0.0 langchain-0.1.8 langchain-community-0.0.21 langchain-core-0.1.25 langchainhub-0.1.14 langdetect-1.0.9 langsmith-0.1.5 marshmallow-3.20.2 mmh3-4.1.0 monotonic-1.6 multiprocess-0.70.16 mypy-extensions-1.0.0 networkx-3.1 ninja-1.10.2.4 nncf-2.8.1 onnx-1.15.0 onnxruntime-1.17.0 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 openvino-nightly-2024.1.0.dev20240221 openvino-telemetry-2023.2.1 optimum-1.17.1 optimum-intel-1.15.1 orjson-3.9.14 overrides-7.7.0 posthog-3.4.2 pulsar-client-3.4.0 pyarrow-15.0.0 pydub-0.25.1 pymoo-0.6.1.1 pypdf-4.0.2 pypika-0.48.9 python-docx-1.1.0 python-dotenv-1.0.1 python-iso639-2024.2.7 python-magic-0.4.27 python-multipart-0.0.9 rapidfuzz-3.6.1 ruff-0.2.2 scikit-learn-1.4.1.post1 semantic-version-2.10.0 sentence_transformers-2.3.1 shellingham-1.5.4 starlette-0.36.3 texttable-1.7.0 tomlkit-0.12.0 transformers-4.37.2 types-requests-2.31.0.20240218 typing-inspect-0.9.0 unstructured-0.12.4 unstructured-client-0.18.0 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## login to huggingfacehub to get access to pretrained model\n",
        "from huggingface_hub import notebook_login, whoami\n",
        "\n",
        "try:\n",
        "    whoami()\n",
        "    print('Authorization token already provided')\n",
        "except OSError:\n",
        "    notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLI1ueDVjqc_",
        "outputId": "a30c7fe1-4456-42a8-9a42-122cf9907fb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorization token already provided\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from optimum.intel import OVQuantizer\n",
        "from optimum.intel.openvino import OVModelForCausalLM\n",
        "import openvino as ov\n",
        "import torch\n",
        "import nncf\n",
        "import logging\n",
        "import shutil\n",
        "import gc\n",
        "import ipywidgets as widgets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    TextIteratorStreamer,\n",
        "    pipeline,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        ")"
      ],
      "metadata": {
        "id": "RdcLv9Mvjudh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508e9bb2-05c1-4524-8f05-16bc8d4db2ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from config import SUPPORTED_EMBEDDING_MODELS, SUPPORTED_LLM_MODELS\n",
        "\n",
        "llm_model_id = list(SUPPORTED_LLM_MODELS)\n",
        "\n",
        "llm_model_id = widgets.Dropdown(\n",
        "    options=llm_model_id,\n",
        "    value=llm_model_id[0],\n",
        "    description=\"LLM Model:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "llm_model_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cf4ada5ada7b44b9b33e442de49d116d",
            "517cb1a415c848a6b70556f69808f0d1",
            "7f8c774fa13b45e2a6b733169314058b"
          ]
        },
        "id": "7AH1hAudjwLL",
        "outputId": "61fb6e81-90a2-4316-9964-54dd1b9c3a2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='LLM Model:', options=('tiny-llama-1b-chat', 'minicpm-2b-dpo', 'red-pajama-3b-chat', 'lla…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf4ada5ada7b44b9b33e442de49d116d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model_configuration = SUPPORTED_LLM_MODELS[llm_model_id.value]\n",
        "print(f\"Selected LLM model {llm_model_id.value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpbw4Bi2jyW-",
        "outputId": "0a14d603-2367-44d7-dcb2-89309efe10d5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected LLM model mistral-7b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "prepare_int4_model = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Prepare INT4 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "prepare_int8_model = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Prepare INT8 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "prepare_fp16_model = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"Prepare FP16 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "display(prepare_int4_model)\n",
        "display(prepare_int8_model)\n",
        "display(prepare_fp16_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109,
          "referenced_widgets": [
            "c33c1237f62c4c64a8fdb7bbe7cb8dd4",
            "cacc0d8ee5304b13b17dfae5c7e247b1",
            "c2f51ea020a941b7b227d98daca6145e",
            "26077142ff754fda868a445a4f1bbb4c",
            "dcc0eb31b4704ba0a9845bf51b9e07b5",
            "81eabc31d5bd46589ab65ed2c610e072",
            "bcb0ad51baba4e8185336430e04f2983",
            "2388e9a1c5484125b2d96876d8c097f0",
            "fe8abf90166c444fbeba7c2213d06d7a"
          ]
        },
        "id": "ZM2LrAKOj4Iz",
        "outputId": "eac33981-cf37-4c67-a19b-8f3873ce5742"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=True, description='Prepare INT4 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c33c1237f62c4c64a8fdb7bbe7cb8dd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=True, description='Prepare INT8 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26077142ff754fda868a445a4f1bbb4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Prepare FP16 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcb0ad51baba4e8185336430e04f2983"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPybLLnCvKel",
        "outputId": "25056300-f9eb-48de-a202-2700d47c9cb0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers_stream_generator in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/dist-packages (from transformers_stream_generator) (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.26.1->transformers_stream_generator) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.26.1->transformers_stream_generator) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from converter import converters\n",
        "\n",
        "nncf.set_log_level(logging.ERROR)\n",
        "\n",
        "pt_model_id = llm_model_configuration[\"model_id\"]\n",
        "pt_model_name = llm_model_id.value.split(\"-\")[0]\n",
        "model_type = AutoConfig.from_pretrained(pt_model_id, trust_remote_code=True).model_type\n",
        "fp16_model_dir = Path(llm_model_id.value) / \"FP16\"\n",
        "int8_model_dir = Path(llm_model_id.value) / \"INT8_compressed_weights\"\n",
        "int4_model_dir = Path(llm_model_id.value) / \"INT4_compressed_weights\"\n",
        "\n",
        "\n",
        "def convert_to_fp16():\n",
        "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    if not llm_model_configuration[\"remote\"]:\n",
        "        ov_model = OVModelForCausalLM.from_pretrained(\n",
        "            pt_model_id, export=True, compile=False, load_in_8bit=False\n",
        "        )\n",
        "        ov_model.half()\n",
        "        ov_model.save_pretrained(fp16_model_dir)\n",
        "        del ov_model\n",
        "    else:\n",
        "        model_kwargs = {}\n",
        "        if \"revision\" in llm_model_configuration:\n",
        "            model_kwargs[\"revision\"] = llm_model_configuration[\"revision\"]\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            llm_model_configuration[\"model_id\"],\n",
        "            torch_dtype=torch.float32,\n",
        "            trust_remote_code=True,\n",
        "            **model_kwargs\n",
        "        )\n",
        "        converters[pt_model_name](model, fp16_model_dir)\n",
        "        del model\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def convert_to_int8():\n",
        "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if not llm_model_configuration[\"remote\"]:\n",
        "        if fp16_model_dir.exists():\n",
        "            ov_model = OVModelForCausalLM.from_pretrained(fp16_model_dir, compile=False, load_in_8bit=False)\n",
        "        else:\n",
        "            ov_model = OVModelForCausalLM.from_pretrained(\n",
        "                pt_model_id, export=True, compile=False\n",
        "            )\n",
        "            ov_model.half()\n",
        "        quantizer = OVQuantizer.from_pretrained(ov_model)\n",
        "        quantizer.quantize(save_directory=int8_model_dir, weights_only=True)\n",
        "        del quantizer\n",
        "        del ov_model\n",
        "    else:\n",
        "        convert_to_fp16()\n",
        "        ov_model = ov.Core().read_model(fp16_model_dir / \"openvino_model.xml\")\n",
        "        shutil.copy(fp16_model_dir / \"config.json\", int8_model_dir / \"config.json\")\n",
        "        configuration_file = fp16_model_dir / f\"configuration_{model_type}.py\"\n",
        "        if configuration_file.exists():\n",
        "            shutil.copy(\n",
        "                configuration_file, int8_model_dir / f\"configuration_{model_type}.py\"\n",
        "            )\n",
        "        compressed_model = nncf.compress_weights(ov_model)\n",
        "        ov.save_model(compressed_model, int8_model_dir / \"openvino_model.xml\")\n",
        "        del ov_model\n",
        "        del compressed_model\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def convert_to_int4():\n",
        "    compression_configs = {\n",
        "        \"zephyr-7b-beta\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"mistral-7b\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"notus-7b-v1\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\",\n",
        "        \"neural-chat-7b-v3-1\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        \"llama-2-chat-7b\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "        \"chatglm2-6b\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.72\n",
        "        },\n",
        "        \"qwen-7b-chat\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.6\n",
        "        },\n",
        "        'red-pajama-3b-chat': {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_ASYM,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.5,\n",
        "        },\n",
        "        \"default\": {\n",
        "            \"mode\": nncf.CompressWeightsMode.INT4_ASYM,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    model_compression_params = compression_configs.get(\n",
        "        llm_model_id.value, compression_configs[\"default\"]\n",
        "    )\n",
        "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    int4_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if not llm_model_configuration[\"remote\"]:\n",
        "        if not fp16_model_dir.exists():\n",
        "            model = OVModelForCausalLM.from_pretrained(\n",
        "                pt_model_id, export=True, compile=False, load_in_8bit=False\n",
        "            ).half()\n",
        "            model.config.save_pretrained(int4_model_dir)\n",
        "            ov_model = model._original_model\n",
        "            del model\n",
        "            gc.collect()\n",
        "        else:\n",
        "            ov_model = ov.Core().read_model(fp16_model_dir / \"openvino_model.xml\")\n",
        "            shutil.copy(fp16_model_dir / \"config.json\", int4_model_dir / \"config.json\")\n",
        "\n",
        "    else:\n",
        "        convert_to_fp16()\n",
        "        ov_model = ov.Core().read_model(fp16_model_dir / \"openvino_model.xml\")\n",
        "        shutil.copy(fp16_model_dir / \"config.json\", int4_model_dir / \"config.json\")\n",
        "        configuration_file = fp16_model_dir / f\"configuration_{model_type}.py\"\n",
        "        if configuration_file.exists():\n",
        "            shutil.copy(\n",
        "                configuration_file, int4_model_dir / f\"configuration_{model_type}.py\"\n",
        "            )\n",
        "    compressed_model = nncf.compress_weights(ov_model, **model_compression_params)\n",
        "    ov.save_model(compressed_model, int4_model_dir / \"openvino_model.xml\")\n",
        "    del ov_model\n",
        "    del compressed_model\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "if prepare_fp16_model.value:\n",
        "    convert_to_fp16()\n",
        "if prepare_int8_model.value:\n",
        "    convert_to_int8()\n",
        "if prepare_int4_model.value:\n",
        "    convert_to_int4()"
      ],
      "metadata": {
        "id": "0OpieR80j64T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "7a292ada-3311-4324-f292-b5f723bb1561"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "This architecture : mistral was not validated, only :pegasus, opt, gpt2, marian, bart, gpt-neox, blenderbot, bloom, gpt-bigcode, llama, gpt-neo, codegen, blenderbot-small architectures were validated, use at your own risk.\n",
            "Framework not specified. Using pt to export the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5332d7b40846>\u001b[0m in \u001b[0;36m<cell line: 154>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprepare_fp16_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mconvert_to_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprepare_int8_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mconvert_to_int8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-5332d7b40846>\u001b[0m in \u001b[0;36mconvert_to_fp16\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mllm_model_configuration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"remote\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         ov_model = OVModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mpt_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mfrom_pretrained_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_transformers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexport\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         return from_pretrained_method(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/intel/openvino/modeling_decoder.py\u001b[0m in \u001b[0;36m_from_transformers\u001b[0;34m(cls, model_id, config, use_auth_token, revision, force_download, cache_dir, subfolder, local_files_only, task, use_cache, trust_remote_code, load_in_8bit, quantization_config, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mstateful\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stateful\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_stateful_is_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         main_export(\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/exporters/openvino/__main__.py\u001b[0m in \u001b[0;36mmain_export\u001b[0;34m(model_name_or_path, output, task, device, framework, cache_dir, trust_remote_code, pad_token_id, subfolder, revision, force_download, local_files_only, use_auth_token, model_kwargs, custom_onnx_configs, fn_get_submodels, compression_option, compression_ratio, stateful, convert_tokenizer, library_name, **kwargs_shapes)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0;34mf\"Asked to export a {model_type} model for the task {task}{autodetected_message}, but the Optimum OpenVINO exporter only supports the tasks {', '.join(model_tasks.keys())} for {model_type}. Please use a supported task. Please open an issue at https://github.com/huggingface/optimum/issues if you would like the task {task} to be supported in the ONNX export for {model_type}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             )\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_transformers_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4.36\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSDPA_ARCHS_ONNX_EXPORT_NOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mloading_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attn_implementation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eager\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/intel/utils/import_utils.py\u001b[0m in \u001b[0;36mis_transformers_version\u001b[0;34m(operation, version)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_transformers_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompare_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transformers_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/packaging/version.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(version)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mraises\u001b[0m \u001b[0mInvalidVersion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/packaging/version.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, version)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Validate the version and parse it into pieces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid version: '{version}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_model_dir = Path(llm_model_id.value) / \"FP16\"\n",
        "int8_model_dir = Path(llm_model_id.value) / \"INT8_compressed_weights\"\n",
        "int4_model_dir = Path(llm_model_id.value) / \"INT4_compressed_weights\""
      ],
      "metadata": {
        "id": "TBBwWOqStjE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
        "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
        "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "if fp16_weights.exists():\n",
        "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
        "    if compressed_weights.exists():\n",
        "        print(\n",
        "            f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\"\n",
        "        )\n",
        "    if compressed_weights.exists() and fp16_weights.exists():\n",
        "        print(\n",
        "            f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVC3rvytkENZ",
        "outputId": "42fc8d8d-c60d-4906-de01-ace3b88eeed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of model with INT8 compressed weights is 6432.58 MB\n",
            "Size of model with INT4 compressed weights is 4042.12 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_id = list(SUPPORTED_EMBEDDING_MODELS)\n",
        "\n",
        "if \"qwen\" not in llm_model_id.value and \"chatglm\" not in llm_model_id.value:\n",
        "    embedding_model_id = [x for x in embedding_model_id if \"chinese\" not in x]\n",
        "\n",
        "embedding_model_id = widgets.Dropdown(\n",
        "    options=embedding_model_id,\n",
        "    value=embedding_model_id[0],\n",
        "    description=\"Embedding Model:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "embedding_model_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1c562f326928442781ebae6a29219168",
            "edb4c0f3e61a44c5ad376d2fdadcf5c3",
            "e2c63d2bd734489299cf38a978845fa0"
          ]
        },
        "id": "4EtjQ1-qkGzL",
        "outputId": "037793b6-0e8c-4ab8-89a7-01bce0170a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Embedding Model:', options=('all-mpnet-base-v2',), value='all-mpnet-base-v2')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c562f326928442781ebae6a29219168"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[embedding_model_id.value]\n",
        "print(f\"Selected {embedding_model_id.value} model\")"
      ],
      "metadata": {
        "id": "39zjp59kkIs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb7456e-fb48-409e-ee9b-9e48ddbce5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected all-mpnet-base-v2 model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_dir = Path(embedding_model_id.value)\n",
        "\n",
        "if not (embedding_model_dir / \"openvino_model.xml\").exists():\n",
        "    model = AutoModel.from_pretrained(embedding_model_configuration[\"model_id\"])\n",
        "    converters[embedding_model_id.value](model, embedding_model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(embedding_model_configuration[\"model_id\"])\n",
        "    tokenizer.save_pretrained(embedding_model_dir)\n",
        "    del model"
      ],
      "metadata": {
        "id": "Bv1eJ5whkKpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "core = ov.Core()\n",
        "embedding_device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value=\"CPU\",\n",
        "    description=\"Device:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "embedding_device"
      ],
      "metadata": {
        "id": "0Y9SNS4XkMYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "67b3521cf9b948d5aaf6d26eccc717c9",
            "04381284025041d398893a622ca44d0f",
            "d1605a1e850a4368b980a614ec593f3c"
          ]
        },
        "outputId": "887be7ed-06f7-4b54-d6a5-3a75edcad2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67b3521cf9b948d5aaf6d26eccc717c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Embedding model will be loaded to {embedding_device.value} device for response generation\")"
      ],
      "metadata": {
        "id": "0eUMSZZbkQa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da863711-b742-4ea0-db47-cb7e10dc819a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model will be loaded to CPU device for response generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value=\"CPU\",\n",
        "    description=\"Device:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "llm_device"
      ],
      "metadata": {
        "id": "Ek2pg36vkSR-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "70a540eb1b1e46678ce80eabb4012581",
            "5e34f8b4019b40c091c6598b8fed852f",
            "948dc72ed7b046869891e10cba1628d3"
          ]
        },
        "outputId": "5a0e10b8-d206-4293-9c41-402049533232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70a540eb1b1e46678ce80eabb4012581"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"LLM model will be loaded to {llm_device.value} device for response generation\")"
      ],
      "metadata": {
        "id": "YiBY41c6kUau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71ab584-b83e-456a-f8f3-9aa9c08e21aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM model will be loaded to CPU device for response generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ov_embedding_model import OVEmbeddings\n",
        "\n",
        "embedding = OVEmbeddings.from_model_id(\n",
        "    embedding_model_dir,\n",
        "    do_norm=embedding_model_configuration[\"do_norm\"],\n",
        "    ov_config={\n",
        "        \"device_name\": embedding_device.value,\n",
        "        \"config\": {\"PERFORMANCE_HINT\": \"THROUGHPUT\"},\n",
        "    },\n",
        "    model_kwargs={\n",
        "        \"model_max_length\": 512,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "YDosxmJXkWeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ov_llm_model import model_classes"
      ],
      "metadata": {
        "id": "IRXUOwb1kYGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int4_model_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sisaL_QxsW3_",
        "outputId": "64f1ce2d-3a79-4f4f-9129-1b47f9db4a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('llama-2-chat-7b/INT4_compressed_weights')"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "available_models = []\n",
        "if int4_model_dir.exists():\n",
        "    available_models.append(\"INT4\")\n",
        "if int8_model_dir.exists():\n",
        "    available_models.append(\"INT8\")\n",
        "if fp16_model_dir.exists():\n",
        "    available_models.append(\"FP16\")\n",
        "\n",
        "model_to_run = widgets.Dropdown(\n",
        "    options=available_models,\n",
        "    value=available_models[0],\n",
        "    description=\"Model to run:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_to_run"
      ],
      "metadata": {
        "id": "KKKkAxLLkZwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3d7d7b6b6acb4e318d093e09253f4be1",
            "2e676ae7ee4e4639ad218d7d9a01592c",
            "ecde56378d3440c682319f05d593d840"
          ]
        },
        "outputId": "102ee07f-787e-4aa8-ae34-11ba97473638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model to run:', options=('INT4', 'INT8'), value='INT4')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d7d7b6b6acb4e318d093e09253f4be1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "if model_to_run.value == \"INT4\":\n",
        "    model_dir = int4_model_dir\n",
        "elif model_to_run.value == \"INT8\":\n",
        "    model_dir = int8_model_dir\n",
        "else:\n",
        "    model_dir = fp16_model_dir\n",
        "print(f\"Loading model from {model_dir}\")\n",
        "\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "# On a GPU device a model is executed in FP16 precision. For red-pajama-3b-chat model there known accuracy\n",
        "# issues caused by this, which we avoid by setting precision hint to \"f32\".\n",
        "if llm_model_id.value == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and llm_device.value in [\"GPU\", \"AUTO\"]:\n",
        "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
        "\n",
        "model_name = llm_model_configuration[\"model_id\"]\n",
        "stop_tokens = llm_model_configuration.get(\"stop_tokens\")\n",
        "class_key = llm_model_id.value.split(\"-\")[0]\n",
        "tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __init__(self, token_ids):\n",
        "        self.token_ids = token_ids\n",
        "\n",
        "    def __call__(\n",
        "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n",
        "    ) -> bool:\n",
        "        for stop_id in self.token_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "if stop_tokens is not None:\n",
        "    if isinstance(stop_tokens[0], str):\n",
        "        stop_tokens = tok.convert_tokens_to_ids(stop_tokens)\n",
        "\n",
        "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
        "\n",
        "model_class = (\n",
        "    OVModelForCausalLM\n",
        "    if not llm_model_configuration[\"remote\"]\n",
        "    else model_classes[class_key]\n",
        ")\n",
        "ov_model = model_class.from_pretrained(\n",
        "    model_dir,\n",
        "    device=llm_device.value,\n",
        "    ov_config=ov_config,\n",
        "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "8K7vRJq9kb6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2cb1c8-0fab-480d-e13b-7333b21005dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from llama-2-chat-7b/INT8_compressed_weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
            "Compiling the model to CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextIteratorStreamer(\n",
        "    tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True\n",
        ")\n",
        "generate_kwargs = dict(\n",
        "    model=ov_model,\n",
        "    tokenizer=tok,\n",
        "    max_new_tokens=128,\n",
        "    streamer=streamer,\n",
        "    temperature=1,\n",
        "    do_sample=True,\n",
        "    top_p=0.8,\n",
        "    top_k=20,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "if stop_tokens is not None:\n",
        "    generate_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", **generate_kwargs)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "KGMb1Y3lkd14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBUdWCFoj2LR",
        "outputId": "fb3182de-6f09-44ad-fd4a-cb0c4932c3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20231228)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader,\n",
        "    PDFMinerLoader,\n",
        "    TextLoader\n",
        ")\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "DEFAULT_CHUNK_SIZE = 1000\n",
        "DEFAULT_CHUNK_OVERLAP = 200\n",
        "DEFAULT_VECTOR_SEARCH_TOP_K = 10\n",
        "DEFAULT_TEXT_TO_PROCESS = 'synopsis.txt'\n",
        "\n",
        "documents = []\n",
        "loader = TextLoader(DEFAULT_TEXT_TO_PROCESS, encoding=\"utf8\")\n",
        "documents.extend(loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "db = FAISS.from_documents(texts, embedding)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": DEFAULT_VECTOR_SEARCH_TOP_K})\n",
        "\n",
        "\n",
        "prompt = PromptTemplate.from_template(llm_model_configuration[\"prompt_template\"])\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        ")\n",
        "\n",
        "print(\"Retriever is Ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9eWaa1czcPE",
        "outputId": "6e098d2f-0b84-47b4-88c9-731ddcc13e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever is Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.run('how much money did mathilda receive at the end?'))"
      ],
      "metadata": {
        "id": "YcVKmYAkpIwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa65a39-fa76-4006-9605-af9feae7c111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Based on the provided context, Mathilda received $100 as an allowance from Tony at the end.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader,\n",
        "    PDFMinerLoader,\n",
        "    TextLoader\n",
        ")\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "DEFAULT_CHUNK_SIZE = 1000\n",
        "DEFAULT_CHUNK_OVERLAP = 200\n",
        "DEFAULT_VECTOR_SEARCH_TOP_K = 4\n",
        "DEFAULT_TEXT_TO_PROCESS = 'xeon.pdf'\n",
        "\n",
        "documents = []\n",
        "loader = PDFMinerLoader(DEFAULT_TEXT_TO_PROCESS)\n",
        "documents.extend(loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "db = FAISS.from_documents(texts, embedding)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": DEFAULT_VECTOR_SEARCH_TOP_K})\n",
        "\n",
        "\n",
        "prompt = PromptTemplate.from_template(llm_model_configuration[\"prompt_template\"])\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        ")\n",
        "\n",
        "print(\"Retriever is Ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxEZuYtardXt",
        "outputId": "6099e1c2-4661-4c91-90ae-55613707faee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever is Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.run('what is intel DLB in new xeon?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRwwWMWJoOfb",
        "outputId": "b5c79402-2afb-43f0-ba41-09bdc546e91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Intel DLB stands for Deep Learning Boost in the context of 4th Gen Intel Xeon Scalable processors. It is a technology that enables improved performance for deep learning workloads by leveraging the power of built-in accelerators.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zSXK1ZxWtEbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}