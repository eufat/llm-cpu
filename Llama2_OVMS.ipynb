{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1h7WDPu1xqX",
        "outputId": "563f50a7-a02b-4e46-bb07-ddf77d476a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.2.1+cpu\n",
            "Uninstalling torch-2.2.1+cpu:\n",
            "  Successfully uninstalled torch-2.2.1+cpu\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cpu/torch-2.2.1%2Bcpu-cp310-cp310-linux_x86_64.whl (186.8 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "optimum 1.17.1 requires transformers[sentencepiece]>=4.26.0, which is not installed.\n",
            "optimum-intel 1.16.0.dev0 requires transformers<4.39.0,>=4.36.0, which is not installed.\n",
            "sentence-transformers 2.5.1 requires transformers<5.0.0,>=4.32.0, which is not installed.\n",
            "torchaudio 2.1.0+cpu requires torch==2.1.0, but you have torch 2.2.1+cpu which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.1+cpu which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.1+cpu which is incompatible.\n",
            "torchvision 0.16.0+cpu requires torch==2.1.0, but you have torch 2.2.1+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.1+cpu\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tritonclient[all] in /usr/local/lib/python3.10/dist-packages (2.43.0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "Requirement already satisfied: ovmsclient in /usr/local/lib/python3.10/dist-packages (2023.1)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (1.23.5)\n",
            "Requirement already satisfied: python-rapidjson>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (1.16)\n",
            "Requirement already satisfied: urllib3>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (2.0.7)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (1.60.1)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (23.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (3.20.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (3.9.3)\n",
            "Requirement already satisfied: geventhttpclient<=2.0.2,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (2.0.2)\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.10/dist-packages (from tritonclient[all]) (12.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[all]) (4.0.3)\n",
            "Requirement already satisfied: gevent>=0.13 in /usr/local/lib/python3.10/dist-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (24.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (2024.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (1.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.10/dist-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (1.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.10/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (5.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (6.2)\n",
            "Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (3.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.event->gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]) (67.7.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: transformers\n",
            "Successfully installed transformers-4.38.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torch transformers\n",
        "!pip install torch  --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install -U tritonclient[all] transformers ovmsclient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login, whoami\n",
        "\n",
        "try:\n",
        "    whoami()\n",
        "    print('Authorization token already provided')\n",
        "except OSError:\n",
        "    notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpDenEf89qZp",
        "outputId": "82d0e8c4-ee35-4d84-d16d-77a9ac803df8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:98: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Error: User cancelled dialog'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorization token already provided\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tritonclient.grpc as grpcclient\n",
        "\n",
        "client = grpcclient.InferenceServerClient(\"10.208.15.235:8080\")\n",
        "\n",
        "server_live = client.is_server_live()\n",
        "server_ready = client.is_server_ready()\n",
        "model_ready = client.is_model_ready(\"llm-llama\")\n",
        "\n",
        "print(\"server_live:\", server_live)\n",
        "print(\"server_ready:\", server_ready)\n",
        "print(\"model_ready:\", model_ready)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKXZ-Kdr1_gp",
        "outputId": "2c8fd908-a1ee-406a-eaf1-c1e2986c9fad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "server_live: True\n",
            "server_ready: True\n",
            "model_ready: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_metadata = client.get_model_metadata(\"llm-llama\")\n",
        "print(model_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8XCXEto2Jt1",
        "outputId": "e1aed7c0-6315-402d-fc3f-b98503ec5c5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: \"llm-llama\"\n",
            "versions: \"1\"\n",
            "platform: \"OpenVINO\"\n",
            "inputs {\n",
            "  name: \"attention_mask\"\n",
            "  datatype: \"INT64\"\n",
            "  shape: -1\n",
            "  shape: -1\n",
            "}\n",
            "inputs {\n",
            "  name: \"beam_idx\"\n",
            "  datatype: \"INT32\"\n",
            "  shape: -1\n",
            "}\n",
            "inputs {\n",
            "  name: \"input_ids\"\n",
            "  datatype: \"INT64\"\n",
            "  shape: -1\n",
            "  shape: -1\n",
            "}\n",
            "inputs {\n",
            "  name: \"position_ids\"\n",
            "  datatype: \"INT64\"\n",
            "  shape: -1\n",
            "  shape: -1\n",
            "}\n",
            "outputs {\n",
            "  name: \"logits\"\n",
            "  datatype: \"FP32\"\n",
            "  shape: -1\n",
            "  shape: -1\n",
            "  shape: 256000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from ovmsclient import make_grpc_client\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "SEED = 0\n",
        "URL = \"10.208.15.235:8080\"\n",
        "ACTOR = \"general-knowledge\"\n",
        "QUESTION = \"describe barack obama\"\n",
        "\n",
        "np.random.seed(SEED)\n",
        "client = make_grpc_client(URL)\n",
        "\n",
        "GENERAL_PRE_PROMPT = \"\"\"You are super-human who is able to answer any question. All questions and answers should end with [EOS].\n",
        "Question: What is the capital of Poland? [EOS]\n",
        "Answer: Warsaw. [EOS]\n",
        "\n",
        "Question: <Q> [EOS]\n",
        "Answer:\"\"\"\n",
        "\n",
        "PROGRAMMER_PRE_PROMPT = \"\"\"You are super-programmer who is brilliant python coder. All the answers contain code snippets ending with [EOS].\n",
        "Question: Write short python function to sum numbers in a list. [EOS]\n",
        "Answer:\n",
        "def sum_all_numbers(numbers):\n",
        "  result = 0\n",
        "    for num in numbers:\n",
        "        result += num\n",
        "    return result [EOS]\n",
        "\n",
        "Question: <Q> [EOS]\n",
        "Answer:\"\"\"\n",
        "\n",
        "if ACTOR == 'general-knowledge':\n",
        "    PRE_PROMPT = GENERAL_PRE_PROMPT\n",
        "else:\n",
        "    PRE_PROMPT = PROGRAMMER_PRE_PROMPT\n",
        "\n",
        "PRE_PROMPT = PRE_PROMPT.replace(\"<Q>\", QUESTION)\n",
        "\n",
        "def convert_tensors_to_numpy(inputs):\n",
        "    result = {}\n",
        "    for key, tensor in inputs.items():\n",
        "        result[key] = tensor.cpu().numpy()  # Move to CPU and convert to NumPy\n",
        "    return result\n",
        "\n",
        "def prepare_preprompt_kv_cache(preprompt):\n",
        "    res = tokenizer(preprompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "    sequence_length = res.input_ids.size(1)\n",
        "    position_ids = torch.arange(sequence_length).unsqueeze(0).long()\n",
        "\n",
        "    beam_width = 4  # Adjust based on your beam search parameters\n",
        "    beam_idx = torch.arange(sequence_length).unsqueeze(0).repeat(beam_width, 1)\n",
        "\n",
        "    inputs = dict(\n",
        "        input_ids = res.input_ids,                  # [1,X]\n",
        "        attention_mask = res.attention_mask,        # [1,X]\n",
        "        beam_idx = beam_idx,\n",
        "        position_ids = position_ids\n",
        "    )\n",
        "    # for i in range(32):\n",
        "    #     inputs[f\"past_key_values.{i}.key\"] = np.zeros((1, 32, 0, 128), dtype=np.float32)\n",
        "    #     inputs[f\"past_key_values.{i}.value\"] = np.zeros((1, 32, 0, 128), dtype=np.float32)\n",
        "\n",
        "    inputs = convert_tensors_to_numpy(inputs)\n",
        "\n",
        "    inputs[\"beam_idx\"] = inputs[\"beam_idx\"].astype(np.int32)\n",
        "\n",
        "    return client.predict(inputs=inputs, model_name='llm-llama')\n",
        "\n",
        "\n",
        "def generate_next_inputs(previous_result, number_of_previous_tokens):\n",
        "    probs = previous_result['logits'][0, -1, :]             # 1,N,32000\n",
        "\n",
        "    probs = np.exp(probs)/sum(np.exp(probs))                # softmax\n",
        "    next_token = np.random.choice(len(probs), p=probs)\n",
        "\n",
        "\n",
        "    next_inputs = dict(\n",
        "        input_ids = np.array([next_token], dtype=np.int64).reshape((1,1)),\n",
        "        attention_mask =np.ones((1, number_of_previous_tokens+1), dtype=np.int64)\n",
        "    )\n",
        "    for j in range(32):\n",
        "        next_inputs[f\"past_key_values.{j}.key\"] = previous_result[f\"present.{j}.key\"]\n",
        "        next_inputs[f\"past_key_values.{j}.value\"] = previous_result[f\"present.{j}.value\"]\n",
        "    return next_inputs, next_token"
      ],
      "metadata": {
        "id": "8XiCTDh25oKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', trust_remote_code=True)\n",
        "actual_token_list = list(tokenizer(PRE_PROMPT, return_tensors=\"pt\", add_special_tokens=False).input_ids[0])\n",
        "content_so_far = PRE_PROMPT\n",
        "\n",
        "results = prepare_preprompt_kv_cache(PRE_PROMPT)\n",
        "while True:\n",
        "    next_inputs, token = generate_next_inputs(results, len(actual_token_list))\n",
        "    results = client.predict(inputs=next_inputs, model_name='llama')\n",
        "\n",
        "    actual_token_list.append(token)\n",
        "    actual_content = tokenizer.decode(actual_token_list)\n",
        "\n",
        "    print(actual_content.replace(content_so_far, ''), end='', flush=True)\n",
        "    content_so_far = actual_content\n",
        "\n",
        "    if actual_content.endswith(\"[EOS]\"):\n",
        "        break"
      ],
      "metadata": {
        "id": "cNeNkHj4BLSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}