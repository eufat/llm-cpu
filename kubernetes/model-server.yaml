apiVersion: intel.com/v1alpha1
kind: ModelServer
metadata:
  name: ovms-llm
  namespace: colab-llm-cpu
spec:
  image_name: openvino/model_server:latest
  deployment_parameters:
    replicas: 1
    openshift_service_mesh: false
    resources:
      limits:
        cpu: "1"
        memory: "10Gi"
      requests:
        cpu: "1"
        memory: "10Gi"
    node_affinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: cloud.google.com/gke-nodepool
              operator: In
              values:
              - large-pool-c3-spot
  models_settings:
    single_model_mode: true
    config_configmap_name: ""
    config_path: ""
    model_name: "llm-llama"
    model_path: "gs://ovms-models/llama2"
    nireq: 0
    plugin_config: '{"CPU_THROUGHPUT_STREAMS":"1"}'
    batch_size: ""
    shape: ""
    model_version_policy: "{\"latest\": {\"num_versions\":1 }}"
    layout: ""
    target_device: "CPU"
    is_stateful: false
    idle_sequence_cleanup: false
    low_latency_transformation: true
    max_sequence_number: 0
  server_settings:
    file_system_poll_wait_seconds: 0
    sequence_cleaner_poll_wait_minutes: 0
    log_level: "INFO"
  service_parameters:
    grpc_port: 8080
    rest_port: 8081
    service_type: ClusterIP
  models_repository:
    storage_type: "google"
    gcp_creds_secret_name: gcpcreds
 